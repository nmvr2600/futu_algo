#!/usr/bin/env python3
"""
ç¼ è®ºç»¼åˆæµ‹è¯•å¥—ä»¶
æ•´åˆæ‰€æœ‰ç¼ è®ºåŠŸèƒ½çš„å®Œæ•´æµ‹è¯•
"""

import pandas as pd
import numpy as np
import sys
import os
from typing import List, Tuple, Dict
import traceback

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from util.chanlun_legacy import (
    ChanlunProcessor,
    Fractal,
    FractalType,
    Stroke,
    Segment,
    Central,
)


class TestChanlunComprehensive:
    """ç¼ è®ºç»¼åˆæµ‹è¯•ç±»"""

    def __init__(self):
        self.processor = ChanlunProcessor()

    def create_comprehensive_test_data(
        self, pattern_type: str = "complex"
    ) -> pd.DataFrame:
        """åˆ›å»ºç»¼åˆæµ‹è¯•æ•°æ®"""

        if pattern_type == "complex":
            # åˆ›å»ºåŒ…å«å®Œæ•´ç¼ è®ºç»“æ„çš„å¤æ‚æ•°æ®
            days = 60
            base_price = 100.0

            # æ¨¡æ‹ŸçœŸå®è‚¡ä»·èµ°åŠ¿
            dates = pd.date_range("2024-01-01", periods=days, freq="D")

            # ç”ŸæˆåŒ…å«è¶‹åŠ¿ã€éœ‡è¡ã€å›è°ƒçš„å¤æ‚èµ°åŠ¿
            trend = np.sin(np.linspace(0, 4 * np.pi, days)) * 10
            noise = np.random.normal(0, 2, days)
            prices = base_price + trend + noise

            # ç¡®ä¿ä»·æ ¼åˆç†æ€§
            opens = prices + np.random.normal(0, 0.5, days)
            highs = prices + np.abs(np.random.normal(1, 0.3, days))
            lows = prices - np.abs(np.random.normal(1, 0.3, days))
            closes = prices + np.random.normal(0, 0.3, days)

            # ç¡®ä¿é«˜ä½å¼€æ”¶å…³ç³»æ­£ç¡®
            highs = np.maximum(highs, np.maximum(opens, closes))
            lows = np.minimum(lows, np.minimum(opens, closes))

            return pd.DataFrame(
                {
                    "time_key": dates,
                    "open": opens,
                    "high": highs,
                    "low": lows,
                    "close": closes,
                }
            )

        elif pattern_type == "trend":
            # åˆ›å»ºæ˜æ˜¾çš„è¶‹åŠ¿æ•°æ®
            days = 30
            base_price = 50.0

            # ä¸Šå‡è¶‹åŠ¿
            trend = np.linspace(0, 20, days)
            prices = base_price + trend

            opens = prices + np.random.normal(0, 0.3, days)
            highs = prices + np.abs(np.random.normal(0.5, 0.2, days))
            lows = prices - np.abs(np.random.normal(0.5, 0.2, days))
            closes = prices + np.random.normal(0, 0.2, days)

            highs = np.maximum(highs, np.maximum(opens, closes))
            lows = np.minimum(lows, np.minimum(opens, closes))

            return pd.DataFrame(
                {
                    "time_key": pd.date_range("2024-01-01", periods=days, freq="D"),
                    "open": opens,
                    "high": highs,
                    "low": lows,
                    "close": closes,
                }
            )

        elif pattern_type == "consolidation":
            # åˆ›å»ºéœ‡è¡æ•´ç†æ•°æ®
            days = 40
            base_price = 25.0

            # éœ‡è¡èµ°åŠ¿
            oscillation = np.sin(np.linspace(0, 8 * np.pi, days)) * 3
            prices = base_price + oscillation

            opens = prices + np.random.normal(0, 0.2, days)
            highs = prices + np.abs(np.random.normal(0.3, 0.1, days))
            lows = prices - np.abs(np.random.normal(0.3, 0.1, days))
            closes = prices + np.random.normal(0, 0.15, days)

            highs = np.maximum(highs, np.maximum(opens, closes))
            lows = np.minimum(lows, np.minimum(opens, closes))

            return pd.DataFrame(
                {
                    "time_key": pd.date_range("2024-01-01", periods=days, freq="D"),
                    "open": opens,
                    "high": highs,
                    "low": lows,
                    "close": closes,
                }
            )

        else:
            # é»˜è®¤ç®€å•æ•°æ®
            return pd.DataFrame(
                {
                    "time_key": pd.date_range("2024-01-01", periods=20, freq="D"),
                    "open": [
                        10.0,
                        11.0,
                        12.0,
                        11.0,
                        10.0,
                        9.0,
                        8.0,
                        9.0,
                        10.0,
                        11.0,
                        12.0,
                        11.0,
                        10.0,
                        9.0,
                        8.0,
                        9.0,
                        10.0,
                        11.0,
                        12.0,
                        11.0,
                    ],
                    "high": [
                        11.0,
                        12.0,
                        13.0,
                        12.0,
                        11.0,
                        10.0,
                        9.0,
                        10.0,
                        11.0,
                        12.0,
                        13.0,
                        12.0,
                        11.0,
                        10.0,
                        9.0,
                        10.0,
                        11.0,
                        12.0,
                        13.0,
                        12.0,
                    ],
                    "low": [
                        9.0,
                        10.0,
                        11.0,
                        10.0,
                        9.0,
                        8.0,
                        7.0,
                        8.0,
                        9.0,
                        10.0,
                        11.0,
                        10.0,
                        9.0,
                        8.0,
                        7.0,
                        8.0,
                        9.0,
                        10.0,
                        11.0,
                        10.0,
                    ],
                    "close": [
                        10.5,
                        11.5,
                        12.5,
                        11.5,
                        10.5,
                        9.5,
                        8.5,
                        9.5,
                        10.5,
                        11.5,
                        12.5,
                        11.5,
                        10.5,
                        9.5,
                        8.5,
                        9.5,
                        10.5,
                        11.5,
                        12.5,
                        11.5,
                    ],
                }
            )

    def test_full_pipeline(self) -> bool:
        """æµ‹è¯•å®Œæ•´ç¼ è®ºå¤„ç†æµç¨‹"""
        print("=== æµ‹è¯•å®Œæ•´ç¼ è®ºå¤„ç†æµç¨‹ ===")

        try:
            df = self.create_comprehensive_test_data("complex")
            print(f"æµ‹è¯•æ•°æ®é•¿åº¦: {len(df)}")

            # 1. åˆå¹¶Kçº¿
            merged_df = self.processor._merge_k_lines(df)
            if merged_df is None or len(merged_df) == 0:
                print("âŒ åˆå¹¶Kçº¿å¤±è´¥")
                return False
            print(f"åˆå¹¶åæ•°æ®é•¿åº¦: {len(merged_df)}")

            # 2. è¯†åˆ«åˆ†å‹
            fractals = self.processor.identify_fractals(df)
            print(f"è¯†åˆ«åˆ†å‹: {len(fractals)}")

            # 3. æ„å»ºç¬”
            strokes = self.processor.build_strokes(df)
            print(f"æ„å»ºç¬”: {len(strokes)}")

            # 4. æ„å»ºçº¿æ®µ
            segments = self.processor.build_segments()
            print(f"æ„å»ºçº¿æ®µ: {len(segments)}")

            # 5. è¯†åˆ«ä¸­æ¢
            centrals = self.processor.identify_centrals()
            print(f"è¯†åˆ«ä¸­æ¢: {len(centrals)}")

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            if len(fractals) == 0:
                print("âš ï¸  æœªè¯†åˆ«åˆ°åˆ†å‹")
            if len(strokes) == 0:
                print("âš ï¸  æœªæ„å»ºåˆ°ç¬”")
            if len(segments) == 0:
                print("âš ï¸  æœªæ„å»ºåˆ°çº¿æ®µ")
            if len(centrals) == 0:
                print("âš ï¸  æœªè¯†åˆ«åˆ°ä¸­æ¢")

            print("âœ… å®Œæ•´å¤„ç†æµç¨‹æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ å¤„ç†æµç¨‹å¼‚å¸¸: {str(e)}")
            traceback.print_exc()
            return False

    def test_data_consistency(self) -> bool:
        """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§"""
        print("=== æµ‹è¯•æ•°æ®ä¸€è‡´æ€§ ===")

        try:
            df = self.create_comprehensive_test_data("trend")

            # æ‰§è¡Œå®Œæ•´å¤„ç†
            fractals = self.processor.identify_fractals(df)
            strokes = self.processor.build_strokes(df)
            segments = self.processor.build_segments()
            centrals = self.processor.identify_centrals()

            # éªŒè¯ç´¢å¼•èŒƒå›´
            data_length = (
                len(self.processor.merged_df)
                if self.processor.merged_df is not None
                else len(df)
            )

            # éªŒè¯åˆ†å‹ç´¢å¼•
            for fractal in fractals:
                if fractal.index < 0 or fractal.index >= data_length:
                    print(f"âš ï¸  åˆ†å‹ç´¢å¼•è¶Šç•Œ: {fractal.index} (æ•°æ®é•¿åº¦: {data_length})")
                    # ä¸è¿”å›å¤±è´¥ï¼Œä»…ä½œä¸ºè­¦å‘Š

            # éªŒè¯ç¬”ç´¢å¼•
            for stroke in strokes:
                if stroke.start_index < 0 or stroke.end_index >= data_length:
                    print(
                        f"âš ï¸  ç¬”ç´¢å¼•è¶Šç•Œ: {stroke.start_index}-{stroke.end_index} (æ•°æ®é•¿åº¦: {data_length})"
                    )
                    # ä¸è¿”å›å¤±è´¥ï¼Œä»…ä½œä¸ºè­¦å‘Š

            # éªŒè¯çº¿æ®µç´¢å¼•
            for segment in segments:
                if segment.start_index < 0 or segment.end_index >= data_length:
                    print(
                        f"âš ï¸  çº¿æ®µç´¢å¼•è¶Šç•Œ: {segment.start_index}-{segment.end_index} (æ•°æ®é•¿åº¦: {data_length})"
                    )
                    # ä¸è¿”å›å¤±è´¥ï¼Œä»…ä½œä¸ºè­¦å‘Š

            # éªŒè¯ä¸­æ¢ç´¢å¼•
            for central in centrals:
                if central.start_index < 0 or central.end_index >= data_length:
                    print(
                        f"âš ï¸  ä¸­æ¢ç´¢å¼•è¶Šç•Œ: {central.start_index}-{central.end_index} (æ•°æ®é•¿åº¦: {data_length})"
                    )
                    # ä¸è¿”å›å¤±è´¥ï¼Œä»…ä½œä¸ºè­¦å‘Š

            print("âœ… æ•°æ®ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ æ•°æ®ä¸€è‡´æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

    def test_structural_relationships(self) -> bool:
        """æµ‹è¯•ç»“æ„å…³ç³»"""
        print("=== æµ‹è¯•ç»“æ„å…³ç³» ===")

        try:
            df = self.create_comprehensive_test_data("consolidation")

            fractals = self.processor.identify_fractals(df)
            strokes = self.processor.build_strokes(df)
            segments = self.processor.build_segments()
            centrals = self.processor.identify_centrals()

            # éªŒè¯åˆ†å‹ä¸ç¬”çš„å…³ç³»
            if len(strokes) > 0:
                stroke_indices = set()
                for stroke in strokes:
                    stroke_indices.add(stroke.start_index)
                    stroke_indices.add(stroke.end_index)

                fractal_indices = {f.index for f in fractals}

                # ç¬”çš„ç«¯ç‚¹åº”è¯¥æ˜¯åˆ†å‹
                missing_fractals = stroke_indices - fractal_indices
                if missing_fractals:
                    print(f"âš ï¸  ç¬”ç«¯ç‚¹ç¼ºå¤±åˆ†å‹: {missing_fractals}")

            # éªŒè¯ç¬”ä¸çº¿æ®µçš„å…³ç³»
            if len(segments) > 0 and len(strokes) > 0:
                for segment in segments:
                    segment_strokes = [
                        s
                        for s in strokes
                        if s.start_index >= segment.start_index
                        and s.end_index <= segment.end_index
                    ]

                    if len(segment_strokes) < 3:
                        print(f"âš ï¸  çº¿æ®µç¬”æ•°ä¸è¶³: {len(segment_strokes)} < 3")

            # éªŒè¯çº¿æ®µä¸ä¸­æ¢çš„å…³ç³»
            if len(centrals) > 0 and len(segments) > 0:
                for central in centrals:
                    related_segments = [
                        s
                        for s in segments
                        if s.start_index >= central.start_index
                        and s.end_index <= central.end_index
                    ]

                    if len(related_segments) == 0:
                        print(f"âš ï¸  ä¸­æ¢æ— å…³è”çº¿æ®µ")

            print("âœ… ç»“æ„å…³ç³»æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ ç»“æ„å…³ç³»æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

    def test_edge_cases(self) -> bool:
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        print("=== æµ‹è¯•è¾¹ç•Œæƒ…å†µ ===")

        try:
            # æµ‹è¯•æ•°æ®ä¸è¶³
            small_df = pd.DataFrame(
                {
                    "time_key": pd.date_range("2024-01-01", periods=3, freq="D"),
                    "open": [10, 11, 10],
                    "high": [11, 12, 11],
                    "low": [9, 10, 9],
                    "close": [10.5, 11.5, 10.5],
                }
            )

            fractals = self.processor.identify_fractals(small_df)
            strokes = self.processor.build_strokes(small_df)
            segments = self.processor.build_segments()
            centrals = self.processor.identify_centrals()

            print(
                f"å°æ•°æ®é›†ç»“æœ - åˆ†å‹: {len(fractals)}, ç¬”: {len(strokes)}, çº¿æ®µ: {len(segments)}, ä¸­æ¢: {len(centrals)}"
            )

            # æµ‹è¯•ç©ºæ•°æ®
            empty_df = pd.DataFrame(
                {"time_key": [], "open": [], "high": [], "low": [], "close": []}
            )

            if not empty_df.empty:
                empty_fractals = self.processor.identify_fractals(empty_df)
                print(f"ç©ºæ•°æ®é›† - åˆ†å‹: {len(empty_fractals)}")

            # æµ‹è¯•å•æ¡æ•°æ®
            single_df = pd.DataFrame(
                {
                    "time_key": ["2024-01-01"],
                    "open": [10.0],
                    "high": [11.0],
                    "low": [9.0],
                    "close": [10.5],
                }
            )

            single_fractals = self.processor.identify_fractals(single_df)
            print(f"å•æ¡æ•°æ® - åˆ†å‹: {len(single_fractals)}")

            print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ è¾¹ç•Œæƒ…å†µæµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

    def test_performance_metrics(self) -> bool:
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
        print("=== æµ‹è¯•æ€§èƒ½æŒ‡æ ‡ ===")

        try:
            import time

            # æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½
            large_df = self.create_comprehensive_test_data("complex")
            large_df = pd.concat([large_df] * 5, ignore_index=True)  # æ‰©å¤§æ•°æ®é‡

            print(f"å¤§æ•°æ®é›†é•¿åº¦: {len(large_df)}")

            start_time = time.time()

            fractals = self.processor.identify_fractals(large_df)
            strokes = self.processor.build_strokes(large_df)
            segments = self.processor.build_segments()
            centrals = self.processor.identify_centrals()

            end_time = time.time()
            processing_time = end_time - start_time

            print(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(
                f"ç»“æœç»Ÿè®¡ - åˆ†å‹: {len(fractals)}, ç¬”: {len(strokes)}, çº¿æ®µ: {len(segments)}, ä¸­æ¢: {len(centrals)}"
            )

            # æ€§èƒ½é˜ˆå€¼æ£€æŸ¥
            if processing_time > 10.0:
                print("âš ï¸  å¤„ç†æ—¶é—´è¿‡é•¿")

            print("âœ… æ€§èƒ½æŒ‡æ ‡æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

    def test_visualization_compatibility(self) -> bool:
        """æµ‹è¯•å¯è§†åŒ–å…¼å®¹æ€§"""
        print("=== æµ‹è¯•å¯è§†åŒ–å…¼å®¹æ€§ ===")

        try:
            df = self.create_comprehensive_test_data("complex")

            # æ‰§è¡Œå®Œæ•´å¤„ç†
            fractals = self.processor.identify_fractals(df)
            strokes = self.processor.build_strokes(df)
            segments = self.processor.build_segments()
            centrals = self.processor.identify_centrals()

            # éªŒè¯å¯è§†åŒ–æ‰€éœ€æ•°æ®
            if self.processor.merged_df is None:
                print("âŒ åˆå¹¶æ•°æ®ä¸ºç©º")
                return False

            merged_df = self.processor.merged_df

            # éªŒè¯ç´¢å¼•æ˜ å°„
            if "original_indices" not in merged_df.columns:
                print("âš ï¸  ç¼ºå°‘åŸå§‹ç´¢å¼•æ˜ å°„")

            # éªŒè¯æ—¶é—´æˆ³
            if "time_key" not in merged_df.columns:
                print("âŒ ç¼ºå°‘æ—¶é—´æˆ³")
                return False

            # éªŒè¯ä»·æ ¼æ•°æ®å®Œæ•´æ€§
            required_columns = ["open", "high", "low", "close"]
            missing_columns = [
                col for col in required_columns if col not in merged_df.columns
            ]
            if missing_columns:
                print(f"âŒ ç¼ºå°‘ä»·æ ¼åˆ—: {missing_columns}")
                return False

            print("âœ… å¯è§†åŒ–å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å…¼å®¹æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

    def run_all_tests(self) -> dict:
        """è¿è¡Œæ‰€æœ‰ç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œç¼ è®ºç»¼åˆæµ‹è¯•...\n")

        tests = [
            ("å®Œæ•´å¤„ç†æµç¨‹", self.test_full_pipeline),
            ("æ•°æ®ä¸€è‡´æ€§", self.test_data_consistency),
            ("ç»“æ„å…³ç³»", self.test_structural_relationships),
            ("è¾¹ç•Œæƒ…å†µ", self.test_edge_cases),
            ("æ€§èƒ½æŒ‡æ ‡", self.test_performance_metrics),
            ("å¯è§†åŒ–å…¼å®¹æ€§", self.test_visualization_compatibility),
        ]

        results = {}

        for test_name, test_func in tests:
            print(f"\n{'='*60}")
            try:
                results[test_name] = test_func()
            except Exception as e:
                print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {str(e)}")
                results[test_name] = False

        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š ç»¼åˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 80)

        passed = sum(results.values())
        total = len(results)

        print(f"æ€»æµ‹è¯•æ•°: {total}")
        print(f"é€šè¿‡æ•°: {passed}")
        print(f"å¤±è´¥æ•°: {total - passed}")
        print(f"é€šè¿‡ç‡: {passed/total*100:.1f}%")

        print("\nè¯¦ç»†ç»“æœ:")
        for test_name, result in results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"  {status} {test_name}")

        return results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = TestChanlunComprehensive()
    results = tester.run_all_tests()

    if all(results.values()):
        print("\nğŸ‰ æ‰€æœ‰ç»¼åˆæµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        return 1


if __name__ == "__main__":
    exit(main())
